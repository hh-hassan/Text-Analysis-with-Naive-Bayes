{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading train and test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"traindata.csv\")\n",
    "test= pd.read_csv(\"testdata.csv\")\n",
    "train=train.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4','Unnamed: 4','Unnamed: 5'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science</td>\n",
       "      <td>Outer space is not friendly to life. Extreme t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>Tennis, original name lawn tennis, game in whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>One woman who frequently flew on Southwest was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covid</td>\n",
       "      <td>In December 2019, almost seven years after the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>Any life-forms that somehow find themselves in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text\n",
       "0   science  Outer space is not friendly to life. Extreme t...\n",
       "1    sports  Tennis, original name lawn tennis, game in whi...\n",
       "2  business  One woman who frequently flew on Southwest was...\n",
       "3     covid  In December 2019, almost seven years after the...\n",
       "4   science  Any life-forms that somehow find themselves in..."
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science</td>\n",
       "      <td>He estimates that 1,000-micrometer pellets cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>science</td>\n",
       "      <td>“That’s enough time to potentially get to Mars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>science</td>\n",
       "      <td>How exactly clumps of microbes might get expel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>science</td>\n",
       "      <td>The microbes might get kicked up by small mete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>Someday, if microbial life is ever discovered ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                               text\n",
       "0  science  He estimates that 1,000-micrometer pellets cou...\n",
       "1  science  “That’s enough time to potentially get to Mars...\n",
       "2  science  How exactly clumps of microbes might get expel...\n",
       "3  science  The microbes might get kicked up by small mete...\n",
       "4  science  Someday, if microbial life is ever discovered ..."
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1 = train.text.str.split(expand=True).stack()\n",
    "u2 = test.text.str.split(expand=True).stack()\n",
    "u=u1.append(u2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  0            Outer\n",
      "   1            space\n",
      "   2               is\n",
      "   3              not\n",
      "   4         friendly\n",
      "   5               to\n",
      "   6            life.\n",
      "   7          Extreme\n",
      "   8    temperatures,\n",
      "   9              low\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(u[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=u.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Outer' 'space' 'is' 'not' 'friendly' 'to' 'life.' 'Extreme'\n",
      " 'temperatures,' 'low']\n"
     ]
    }
   ],
   "source": [
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=words.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation)\n",
    "final = [w.translate(table) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Outer', 'space', 'is', 'not', 'friendly', 'to', 'life', 'Extreme', 'temperatures', 'low', 'pressure', 'and', 'radiation', 'can', 'quickly', 'degrade', 'cell', 'membranes', 'and', 'destroy', 'DNA', 'Tennis', 'original', 'name', 'lawn', 'tennis', 'game', 'in', 'which', 'two', 'opposing', 'players', 'singles', 'or', 'pairs', 'of', 'players', 'doubles', 'use', 'tautly', 'strung', 'rackets', 'to', 'hit', 'a', 'ball', 'of', 'specified', 'size', 'weight', 'and', 'bounce', 'over', 'a', 'net', 'on', 'a', 'rectangular', 'court', 'One', 'woman', 'who', 'frequently', 'flew', 'on', 'Southwest', 'was', 'constantly', 'disappointed', 'with', 'every', 'aspect', 'of', 'the', 'company’s', 'operation', 'In', 'fact', 'she', 'became', 'known', 'as', 'the', '“Pen', 'Pal”', 'because', 'after', 'every', 'flight', 'she', 'wrote', 'in', 'with', 'a', 'complaint', 'In', 'December', '2019', 'almost', 'seven']\n"
     ]
    }
   ],
   "source": [
    "print(final[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word.lower() for word in final]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence = [] \n",
    "  \n",
    "for w in words: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1611"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence=np.unique(np.array(filtered_sentence))\n",
    "filtered_sentence_final=[]\n",
    "for word in filtered_sentence:\n",
    "    if not word.isdigit():\n",
    "        filtered_sentence_final.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence=filtered_sentence_final\n",
    "len(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '1000micrometer', '1000micrometers', '100micrometerthick', '12th–13thcentury', '15–30', '1960s', '1970s', '2019ncov', '21st', '30–15', 'abandon', 'ability', 'able', 'abuse', 'abusive', 'accidentally', 'according', 'ace2', 'across', 'activities', 'acute', 'adapt', 'added', 'adds', 'affected', 'affixed', 'agent', 'agreed', 'akihiko', 'allowed', 'allows', 'almost', 'also', 'alternate', 'although', 'always', 'amateurs', 'among', 'ancient', 'andor', 'angiotensinconverting', 'angles', 'animal', 'animals', 'antihiv', 'antivirals', 'anything', 'appeal', 'appeared', 'appears', 'april', 'aren’t', 'argues', 'around', 'arrived', 'artificial', 'asked', 'aspect', 'asphalt', 'assign', 'associations', 'astrobiologist', 'astronaut', 'astronautical', 'attack', 'attempts', 'attendant', 'attitude', 'attracted', 'australia', 'available', 'avian', 'awarded', 'away', 'back', 'bacteria', 'bacteriato', 'bad', 'balances', 'ball', 'balls', 'band', 'baseline', 'bats', 'became', 'began', 'beginning', 'behind', 'bethune', 'betrayals', 'better', 'better”', 'biggest', 'body', 'books', 'boss', 'bounce', 'bounces', 'boundaries', 'britain', 'broadcasts', 'broadened', 'bumped', 'business', 'businesses', 'buy', 'calculation', 'calgary', 'called', 'camel', 'canada', 'cancelled', 'can’t', 'care', 'carry', 'case', 'cases', 'causative', 'cause', 'caused', 'causes', 'cell', 'cells', 'cement', 'centre', 'century', 'ceo', 'championships', 'change', 'changed', 'chess', 'child', 'china', 'chinas', 'choice', 'choose', 'chooses', 'circuits', 'city', 'clay', 'clear', 'cloth', 'clumps', 'cluster', 'clusters', 'cm', 'coat', 'coin', 'coined', 'cold', 'come', 'coming', 'commit', 'common', 'communities', 'company', 'company’s', 'competition', 'competitions', 'complaining', 'complaint', 'complaints', 'complex', 'conditions', 'consists', 'constantly', 'constitute', 'constitutes', 'contain', 'contained', 'containment', 'continental', 'continued', 'continues', 'contract', 'convince', 'core', 'corona', 'coronaviridae', 'coronavirus', 'coronaviruses', 'correct', 'correctly', 'corsets', 'could', 'counterproductive', 'countries', 'course', 'court', 'courts', 'courts”', 'cov', 'covered', 'covid19', 'cramm', 'cruise', 'cup', 'cushioned', 'customer']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_sentence[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=0\n",
    "vocabulary={}\n",
    "for word in filtered_sentence:\n",
    "    vocabulary[word]=z\n",
    "    z=z+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, '1000micrometer': 1, '1000micrometers': 2, '100micrometerthick': 3, '12th–13thcentury': 4, '15–30': 5, '1960s': 6, '1970s': 7, '2019ncov': 8, '21st': 9, '30–15': 10, 'abandon': 11, 'ability': 12, 'able': 13, 'abuse': 14, 'abusive': 15, 'accidentally': 16, 'according': 17, 'ace2': 18, 'across': 19, 'activities': 20, 'acute': 21, 'adapt': 22, 'added': 23, 'adds': 24, 'affected': 25, 'affixed': 26, 'agent': 27, 'agreed': 28, 'akihiko': 29, 'allowed': 30, 'allows': 31, 'almost': 32, 'also': 33, 'alternate': 34, 'although': 35, 'always': 36, 'amateurs': 37, 'among': 38, 'ancient': 39, 'andor': 40, 'angiotensinconverting': 41, 'angles': 42, 'animal': 43, 'animals': 44, 'antihiv': 45, 'antivirals': 46, 'anything': 47, 'appeal': 48, 'appeared': 49, 'appears': 50, 'april': 51, 'aren’t': 52, 'argues': 53, 'around': 54, 'arrived': 55, 'artificial': 56, 'asked': 57, 'aspect': 58, 'asphalt': 59, 'assign': 60, 'associations': 61, 'astrobiologist': 62, 'astronaut': 63, 'astronautical': 64, 'attack': 65, 'attempts': 66, 'attendant': 67, 'attitude': 68, 'attracted': 69, 'australia': 70, 'available': 71, 'avian': 72, 'awarded': 73, 'away': 74, 'back': 75, 'bacteria': 76, 'bacteriato': 77, 'bad': 78, 'balances': 79, 'ball': 80, 'balls': 81, 'band': 82, 'baseline': 83, 'bats': 84, 'became': 85, 'began': 86, 'beginning': 87, 'behind': 88, 'bethune': 89, 'betrayals': 90, 'better': 91, 'better”': 92, 'biggest': 93, 'body': 94, 'books': 95, 'boss': 96, 'bounce': 97, 'bounces': 98, 'boundaries': 99, 'britain': 100, 'broadcasts': 101, 'broadened': 102, 'bumped': 103, 'business': 104, 'businesses': 105, 'buy': 106, 'calculation': 107, 'calgary': 108, 'called': 109, 'camel': 110, 'canada': 111, 'cancelled': 112, 'can’t': 113, 'care': 114, 'carry': 115, 'case': 116, 'cases': 117, 'causative': 118, 'cause': 119, 'caused': 120, 'causes': 121, 'cell': 122, 'cells': 123, 'cement': 124, 'centre': 125, 'century': 126, 'ceo': 127, 'championships': 128, 'change': 129, 'changed': 130, 'chess': 131, 'child': 132, 'china': 133, 'chinas': 134, 'choice': 135, 'choose': 136, 'chooses': 137, 'circuits': 138, 'city': 139, 'clay': 140, 'clear': 141, 'cloth': 142, 'clumps': 143, 'cluster': 144, 'clusters': 145, 'cm': 146, 'coat': 147, 'coin': 148, 'coined': 149, 'cold': 150, 'come': 151, 'coming': 152, 'commit': 153, 'common': 154, 'communities': 155, 'company': 156, 'company’s': 157, 'competition': 158, 'competitions': 159, 'complaining': 160, 'complaint': 161, 'complaints': 162, 'complex': 163, 'conditions': 164, 'consists': 165, 'constantly': 166, 'constitute': 167, 'constitutes': 168, 'contain': 169, 'contained': 170, 'containment': 171, 'continental': 172, 'continued': 173, 'continues': 174, 'contract': 175, 'convince': 176, 'core': 177, 'corona': 178, 'coronaviridae': 179, 'coronavirus': 180, 'coronaviruses': 181, 'correct': 182, 'correctly': 183, 'corsets': 184, 'could': 185, 'counterproductive': 186, 'countries': 187, 'course': 188, 'court': 189, 'courts': 190, 'courts”': 191, 'cov': 192, 'covered': 193, 'covid19': 194, 'cramm': 195, 'cruise': 196, 'cup': 197, 'cushioned': 198, 'customer': 199, 'customers': 200, 'customer’s': 201, 'davis': 202, 'day': 203, 'de': 204, 'dead': 205, 'death': 206, 'december': 207, 'decide': 208, 'declaring': 209, 'defend': 210, 'definition': 211, 'degrade': 212, 'degree': 213, 'deinococcus': 214, 'deliver': 215, 'demand': 216, 'demanding': 217, 'demands': 218, 'department': 219, 'depending': 220, 'derivatives': 221, 'derived': 222, 'desiccation': 223, 'desk': 224, 'destroy': 225, 'detail': 226, 'detected': 227, 'diagonally': 228, 'diameter': 229, 'didn’t': 230, 'die': 231, 'dimensions': 232, 'dipeptidyl': 233, 'disappointed': 234, 'discolored': 235, 'discovered': 236, 'disease': 237, 'diseases': 238, 'dismissed': 239, 'dismissing': 240, 'diverse': 241, 'dna': 242, 'don’t': 243, 'doubles': 244, 'dpp4': 245, 'dream”': 246, 'dried': 247, 'dromedary': 248, 'drugs': 249, 'due': 250, 'earth': 251, 'earth’s': 252, 'east': 253, 'ecological': 254, 'economically': 255, 'eight': 256, 'either': 257, 'electron': 258, 'else': 259, 'emblems': 260, 'employees': 261, 'employees’': 262, 'enjoyed': 263, 'enough': 264, 'entry': 265, 'environment': 266, 'enzyme2': 267, 'epidemic': 268, 'equal': 269, 'estimates': 270, 'europe': 271, 'even': 272, 'ever': 273, 'every': 274, 'evidence': 275, 'evolved': 276, 'exactly': 277, 'examples': 278, 'expanding': 279, 'expelled': 280, 'experience': 281, 'experts': 282, 'explained': 283, 'exploiting': 284, 'extensively': 285, 'exterior': 286, 'extreme': 287, 'extremes': 288, 'fact': 289, 'factors': 290, 'fails': 291, 'failure': 292, 'falling': 293, 'family': 294, 'father': 295, 'favors': 296, 'feature': 297, 'fed': 298, 'federation': 299, 'feet': 300, 'field': 301, 'financial': 302, 'find': 303, 'finding': 304, 'finished': 305, 'fired': 306, 'first': 307, 'five': 308, 'flannels': 309, 'flew': 310, 'flight': 311, 'floating': 312, 'flu': 313, 'flying': 314, 'follow': 315, 'food': 316, 'formally': 317, 'formed': 318, 'forming': 319, 'forms': 320, 'founder': 321, 'four': 322, 'free': 323, 'french': 324, 'frequently': 325, 'fried': 326, 'friendly': 327, 'future': 328, 'gain': 329, 'gained': 330, 'galidesivir': 331, 'game': 332, 'games': 333, 'games—the': 334, 'gardenparty': 335, 'generally': 336, 'genetic': 337, 'gentlemen': 338, 'get': 339, 'give': 340, 'giving': 341, 'globally': 342, 'go': 343, 'goes': 344, 'going': 345, 'good': 346, 'gordon': 347, 'govern': 348, 'governing': 349, 'gradually': 350, 'grams': 351, 'grass': 352, 'great': 353, 'greater': 354, 'greatly': 355, 'ground': 356, 'group': 357, 'group’s': 358, 'group”': 359, 'growth': 360, 'guangdong': 361, 'guy': 362, 'handball': 363, 'happen': 364, 'happened': 365, 'happy': 366, 'harbor': 367, 'hard': 368, 'harder': 369, 'harry': 370, 'harsh': 371, 'hat': 372, 'hazards': 373, 'heart': 374, 'height': 375, 'help': 376, 'helps': 377, 'herb': 378, 'herb’s': 379, 'he’d': 380, 'high': 381, 'highquality': 382, 'hit': 383, 'hits': 384, 'hiv': 385, 'hold': 386, 'home': 387, 'hopes': 388, 'host': 389, 'however': 390, 'hubei': 391, 'human': 392, 'humans': 393, 'hygiene': 394, 'idea': 395, 'identification': 396, 'identified': 397, 'ie': 398, 'illnesses': 399, 'impact': 400, 'inch': 401, 'inches': 402, 'increasing': 403, 'indoor': 404, 'infect': 405, 'infected': 406, 'infection': 407, 'infections': 408, 'inner': 409, 'inside': 410, 'institute': 411, 'interest': 412, 'intermediate': 413, 'international': 414, 'intervals': 415, 'introduce': 416, 'iran': 417, 'irate': 418, 'ironically': 419, 'itf': 420, 'it’s': 421, 'it”': 422, 'iv': 423, 'january': 424, 'japan': 425, 'jerk': 426, 'jerks': 427, 'jeu': 428, 'jobs': 429, 'journey': 430, 'jump': 431, 'keep': 432, 'kelleher': 433, 'kelly': 434, 'kept': 435, 'kicked': 436, 'kid': 437, 'kid’s': 438, 'kkk': 439, 'known': 440, 'ladies': 441, 'lady': 442, 'large': 443, 'larger': 444, 'last': 445, 'late': 446, 'later': 447, 'lawn': 448, 'layers': 449, 'leads': 450, 'learn': 451, 'letter': 452, 'level': 453, 'life': 454, 'lifeforms': 455, 'like': 456, 'liked': 457, 'likely': 458, 'likes': 459, 'limited': 460, 'line': 461, 'litany': 462, 'london': 463, 'long': 464, 'look': 465, 'looked': 466, 'lose': 467, 'loss': 468, 'lousy': 469, 'low': 470, 'loyalty': 471, 'l’oeuf': 472, 'made': 473, 'magnetic': 474, 'mainly': 475, 'maintenance': 476, 'major': 477, 'make': 478, 'makes': 479, 'making': 480, 'man': 481, 'management': 482, 'managers': 483, 'margaret': 484, 'margin': 485, 'mars': 486, 'mars”': 487, 'match': 488, 'material': 489, 'materials': 490, 'matter': 491, 'maxim': 492, 'may': 493, 'means': 494, 'mechanism': 495, 'medalsport': 496, 'medieval': 497, 'membranes': 498, 'men': 499, 'mers': 500, 'merscov': 501, 'metal': 502, 'meteorites': 503, 'meteors': 504, 'metre': 505, 'metres': 506, 'microbes': 507, 'microbial': 508, 'microbiologist': 509, 'microscope': 510, 'middle': 511, 'might': 512, 'million': 513, 'miss': 514, 'mixed': 515, 'moistened': 516, 'momentarily': 517, 'money': 518, 'month': 519, 'months': 520, 'much': 521, 'must': 522, 'mutate': 523, 'mysterious': 524, 'name': 525, 'named': 526, 'nasa': 527, 'national': 528, 'nazi': 529, 'net': 530, 'never': 531, 'new': 532, 'nice': 533, 'notable': 534, 'note': 535, 'not”': 536, 'novel': 537, 'number': 538, 'nylon': 539, 'occur': 540, 'odd': 541, 'offended': 542, 'offensive': 543, 'office': 544, 'officials': 545, 'olympic': 546, 'one': 547, 'one’s': 548, 'opened': 549, 'operation': 550, 'opponent': 551, 'opponents': 552, 'opponent’s': 553, 'opposing': 554, 'options': 555, 'organized': 556, 'origin': 557, 'original': 558, 'originally': 559, 'originate': 560, 'originated': 561, 'originating': 562, 'origins': 563, 'ounces': 564, 'outbreak': 565, 'outbreaks': 566, 'outer': 567, 'outside': 568, 'overlooked': 569, 'oversee': 570, 'pace': 571, 'pairs': 572, 'palm”': 573, 'pal”': 574, 'pandemic': 575, 'paper': 576, 'paris': 577, 'part': 578, 'participant': 579, 'passenger’s': 580, 'paume': 581, 'peanuts': 582, 'pellets': 583, 'people': 584, 'people’': 585, 'peptidase': 586, 'percent': 587, 'period': 588, 'perturbations': 589, 'petticoats': 590, 'phrase': 591, 'physical': 592, 'placed': 593, 'places': 594, 'plain': 595, 'planets': 596, 'plates': 597, 'play': 598, 'played': 599, 'player': 600, 'players': 601, 'plenty': 602, 'pneumonia': 603, 'point': 604, 'points': 605, 'positivesense': 606, 'possess': 607, 'possibly': 608, 'posts': 609, 'potentially': 610, 'practically': 611, 'practices': 612, 'prescribed': 613, 'pressure': 614, 'pressurized': 615, 'pretty': 616, 'princess”': 617, 'problem': 618, 'product': 619, 'professional': 620, 'professionals': 621, 'promptly': 622, 'propelling': 623, 'protein': 624, 'province': 625, 'put': 626, 'quarantine': 627, 'question': 628, 'quickly': 629, 'quite': 630, 'racket': 631, 'racketandball': 632, 'rackets': 633, 'radiation': 634, 'radiationresistant': 635, 'ran': 636, 'ranging': 637, 'rapid': 638, 'rapidly': 639, 'rare': 640, 'rather': 641, 'real': 642, 'receive': 643, 'receiver': 644, 'receptor': 645, 'reciting': 646, 'recommended': 647, 'rectangular': 648, 'reel': 649, 'referred': 650, 'refers': 651, 'region': 652, 'regular': 653, 'rein': 654, 'relations': 655, 'remdesivir': 656, 'renewed': 657, 'replayed': 658, 'replication': 659, 'reports': 660, 'research': 661, 'researchers': 662, 'resentment': 663, 'respiratory': 664, 'restored': 665, 'results': 666, 'return': 667, 'returned': 668, 'rich': 669, 'right': 670, 'righthand': 671, 'right”': 672, 'rise': 673, 'rivalries': 674, 'rna': 675, 'rosenbluth': 676, 'royal': 677, 'rubber': 678, 'rudely': 679, 'rules': 680, 'run': 681, 'said': 682, 'samples': 683, 'sanctioned': 684, 'sars': 685, 'sarscov': 686, 'satisfactorily': 687, 'say': 688, 'says': 689, 'science': 690, 'score': 691, 'scoring': 692, 'scott': 693, 'seams': 694, 'seats': 695, 'second': 696, 'seemed': 697, 'selection': 698, 'selfridge': 699, 'selfridge’s': 700, 'sense': 701, 'sent': 702, 'serfs': 703, 'serve': 704, 'server': 705, 'server’s': 706, 'service': 707, 'servicegruppen': 708, 'setback': 709, 'seven': 710, 'severe': 711, 'severity': 712, 'sheets': 713, 'shielded': 714, 'ship': 715, 'shock': 716, 'shot': 717, 'shows': 718, 'side': 719, 'sides': 720, 'siding': 721, 'significant': 722, 'similarly': 723, 'simply': 724, 'since': 725, 'singles': 726, 'site': 727, 'size': 728, 'skill': 729, 'slogan': 730, 'small': 731, 'smallest': 732, 'snaps': 733, 'socioeconomic': 734, 'soft': 735, 'solve': 736, 'somebody': 737, 'someday': 738, 'somehow': 739, 'sometimes': 740, 'soon': 741, 'sorts': 742, 'southwest': 743, 'southwest’s': 744, 'space': 745, 'specified': 746, 'specifies': 747, 'spectator': 748, 'spin': 749, 'sport': 750, 'spread': 751, 'spreads': 752, 'squarely': 753, 'stamina': 754, 'starched': 755, 'states': 756, 'station': 757, 'status': 758, 'stayed': 759, 'still': 760, 'stitchless': 761, 'store': 762, 'strategic': 763, 'stratosphere': 764, 'strike': 765, 'strikes': 766, 'strokes': 767, 'strung': 768, 'studied': 769, 'studies': 770, 'study': 771, 'stuff': 772, 'stuffed': 773, 'stumped': 774, 'stylistic': 775, 'suggested': 776, 'suggests': 777, 'support': 778, 'supported': 779, 'supporting': 780, 'sure': 781, 'surface': 782, 'surfaced': 783, 'surfaces': 784, 'survive': 785, 'survived': 786, 'suspected': 787, 'sway': 788, 'syndrome': 789, 'synthetic': 790, 'system': 791, 'take': 792, 'task': 793, 'tautly': 794, 'team': 795, 'technical': 796, 'technicians': 797, 'television': 798, 'temperatures': 799, 'tennis': 800, 'term': 801, 'territories': 802, 'test': 803, 'that’s': 804, 'they’re': 805, 'thick': 806, 'thin': 807, 'think': 808, 'though': 809, 'thought': 810, 'three': 811, 'thrive': 812, 'thrown': 813, 'thunderstorminduced': 814, 'thus': 815, 'ticket': 816, 'time': 817, 'today': 818, 'together': 819, 'tokyo': 820, 'told': 821, 'top': 822, 'toss': 823, 'tournament': 824, 'traced': 825, 'tract': 826, 'trade': 827, 'transmission': 828, 'transmitted': 829, 'travel': 830, 'treat': 831, 'treated': 832, 'treatment': 833, 'trip': 834, 'trusted': 835, 'trying': 836, 'two': 837, 'typically': 838, 'ultimate': 839, 'ultraviolet': 840, 'uniform': 841, 'unique': 842, 'united': 843, 'university': 844, 'unless': 845, 'unprotected': 846, 'unreasonable': 847, 'upon': 848, 'us': 849, 'usa': 850, 'use': 851, 'used': 852, 'using': 853, 'usually': 854, 'vaccine': 855, 'value': 856, 'variety': 857, 'victorian': 858, 'viral': 859, 'virus': 860, 'viruses': 861, 'visible': 862, 'void': 863, 'volley': 864, 'waited': 865, 'wanted': 866, 'wants': 867, 'war': 868, 'way': 869, 'weaknesses': 870, 'wear': 871, 'wearing': 872, 'weight': 873, 'well': 874, 'wells': 875, 'went': 876, 'whalebone': 877, 'whenever': 878, 'whereas': 879, 'whether': 880, 'white': 881, 'whose': 882, 'widely': 883, 'win': 884, 'winner': 885, 'within': 886, 'without': 887, 'withstand': 888, 'woman': 889, 'won’t': 890, 'wool': 891, 'word': 892, 'work': 893, 'works': 894, 'world': 895, 'worldwide': 896, 'worry': 897, 'worse': 898, 'would': 899, 'write': 900, 'wrong': 901, 'wrote': 902, 'wuhan': 903, 'yamagishi': 904, 'year': 905, 'years': 906, 'yellow': 907, 'yours’': 908, 'zero': 909, 'zoonotic': 910, '—': 911, '‘fly': 912, '‘this': 913, '“always': 914, '“diamond': 915, '“egg”': 916, '“fault”': 917, '“game': 918, '“hard': 919, '“it': 920, '“let”': 921, '“love”': 922, '“my': 923, '“no': 924, '“no”': 925, '“pen': 926, '“that’s': 927, '“the': 928}\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the prior distribution of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'covid', 'science', 'sports'], dtype=object)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_labels=len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "business=train['category'][train['category']=='business'].count()\n",
    "covid=train['category'][train['category']=='covid'].count()\n",
    "science=train['category'][train['category']=='science'].count()\n",
    "sports=train['category'][train['category']=='sports'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_business=business/total_labels#prior distribution of business\n",
    "pr_covid=covid/total_labels#prior distribution of covid\n",
    "pr_science=science/total_labels#prior distribution of science\n",
    "pr_sports=sports/total_labels#prior distribution of sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2375\n",
      "0.2625\n",
      "0.25\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "print(pr_business)\n",
    "print(pr_covid)\n",
    "print(pr_science)\n",
    "print(pr_sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the train and test text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x=x.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    x = [w.translate(table) for w in x]\n",
    "    x = [word.lower() for word in x]\n",
    "    final = [] \n",
    "    for w in x: \n",
    "        if w not in stop_words: \n",
    "            final.append(w)\n",
    "    x=np.array(final)\n",
    "    filtered_sentence_final=[]\n",
    "    for word in x:\n",
    "        if not word.isdigit():\n",
    "            filtered_sentence_final.append(word)\n",
    "    x=np.unique(filtered_sentence_final)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vocabulary(x):\n",
    "    final=[]\n",
    "    for word in x:\n",
    "        final.append(vocabulary[word])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Outer space is not friendly to life. Extreme temperatures, low pressure and radiation can quickly degrade cell membranes and destroy DNA.'"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cell', 'degrade', 'destroy', 'dna', 'extreme', 'friendly', 'life',\n",
       "       'low', 'membranes', 'outer', 'pressure', 'quickly', 'radiation',\n",
       "       'space', 'temperatures'], dtype='<U12')"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(train.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[122, 212, 225, 242, 287, 327, 454, 470, 498, 567, 614, 629, 634, 745, 799]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vocabulary(preprocess(train.text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He estimates that 1,000-micrometer pellets could survive eight years floating through space.'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1000micrometer', 'could', 'eight', 'estimates', 'floating',\n",
       "       'pellets', 'space', 'survive', 'years'], dtype='<U14')"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(test.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 185, 256, 270, 312, 583, 745, 785, 906]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vocabulary(preprocess(test.text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the class-conditional probabilities of each word in the vocabulary, for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['business', 'covid', 'science', 'sports']\n",
    "class1=[0]*len(vocabulary)\n",
    "class2=[0]*len(vocabulary)\n",
    "class3=[0]*len(vocabulary)\n",
    "class4=[0]*len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(train.category):\n",
    "    x=train.text[i]\n",
    "    words=preprocess(x)\n",
    "    words_vec=word_to_vocabulary(words)\n",
    "    if j==classes[0]:\n",
    "        for x in words_vec:\n",
    "            class1[x]=class1[x]+1\n",
    "    if j==classes[1]:\n",
    "        for x in words_vec:\n",
    "            class2[x]=class2[x]+1\n",
    "    if j==classes[2]:\n",
    "        for x in words_vec:\n",
    "            class3[x]=class3[x]+1\n",
    "    if j==classes[3]:\n",
    "        for x in words_vec:\n",
    "            class4[x]=class4[x]+1\n",
    "class1=class1/business\n",
    "class2=class2/covid\n",
    "class3=class3/science\n",
    "class4=class4/sports\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing zero probability with 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1[class1==0] = 0.1\n",
    "class2[class2==0] = 0.1\n",
    "class3[class3==0] = 0.1\n",
    "class4[class4==0] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10526316 0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.05263158 0.10526316 0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.10526316 0.1        0.1\n",
      " 0.21052632 0.1        0.05263158 0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.1        0.1        0.05263158 0.05263158\n",
      " 0.1        0.05263158 0.1        0.1        0.05263158 0.1\n",
      " 0.05263158 0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.10526316 0.1        0.1\n",
      " 0.21052632 0.05263158 0.1        0.1        0.1        0.1\n",
      " 0.1        0.05263158 0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.10526316 0.05263158 0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.05263158 0.05263158 0.15789474 0.05263158 0.05263158\n",
      " 0.1        0.1        0.1        0.1        0.05263158 0.10526316\n",
      " 0.1        0.1        0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.05263158 0.1        0.1        0.1        0.1\n",
      " 0.1        0.05263158 0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.05263158 0.1        0.1\n",
      " 0.1        0.1        0.1        0.05263158 0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.1\n",
      " 0.15789474 0.05263158 0.1        0.1        0.05263158 0.05263158\n",
      " 0.05263158 0.1        0.05263158 0.1        0.05263158 0.1\n",
      " 0.1        0.1        0.1        0.1        0.05263158 0.1\n",
      " 0.1        0.05263158 0.05263158 0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.05263158 0.1        0.05263158 0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.63157895 0.42105263 0.10526316 0.1        0.10526316\n",
      " 0.1        0.1        0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.05263158 0.1        0.1        0.1        0.1\n",
      " 0.05263158 0.05263158 0.05263158 0.05263158 0.1        0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.1\n",
      " 0.1        0.1        0.10526316 0.1        0.1        0.1\n",
      " 0.05263158 0.1        0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.1        0.42105263 0.05263158 0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.10526316 0.1        0.15789474 0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.10526316 0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.05263158\n",
      " 0.05263158 0.05263158 0.1        0.1        0.05263158 0.05263158\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.05263158 0.1        0.05263158\n",
      " 0.1        0.05263158 0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.10526316 0.15789474 0.05263158\n",
      " 0.1        0.1        0.1        0.05263158 0.05263158 0.05263158\n",
      " 0.1        0.1        0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.05263158 0.1        0.1        0.05263158 0.05263158 0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.05263158 0.05263158 0.1        0.1        0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.1\n",
      " 0.05263158 0.1        0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.10526316 0.05263158\n",
      " 0.1        0.05263158 0.1        0.1        0.1        0.1\n",
      " 0.05263158 0.05263158 0.1        0.05263158 0.1        0.1\n",
      " 0.05263158 0.10526316 0.1        0.05263158 0.1        0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.05263158 0.1\n",
      " 0.1        0.05263158 0.1        0.1        0.1        0.1\n",
      " 0.05263158 0.1        0.05263158 0.1        0.1        0.1\n",
      " 0.21052632 0.05263158 0.1        0.1        0.1        0.05263158\n",
      " 0.05263158 0.05263158 0.05263158 0.1        0.1        0.05263158\n",
      " 0.1        0.05263158 0.1        0.05263158 0.1        0.05263158\n",
      " 0.1        0.1        0.05263158 0.1        0.15789474 0.10526316\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.05263158\n",
      " 0.10526316 0.1        0.05263158 0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.05263158 0.1        0.1\n",
      " 0.1        0.05263158 0.05263158 0.05263158 0.1        0.10526316\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.10526316 0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.05263158 0.1\n",
      " 0.1        0.15789474 0.05263158 0.1        0.05263158 0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.05263158 0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.05263158 0.1\n",
      " 0.1        0.05263158 0.1        0.1        0.1        0.1\n",
      " 0.05263158 0.1        0.21052632 0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.10526316 0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.05263158 0.05263158 0.1        0.1        0.05263158 0.1\n",
      " 0.1        0.1        0.10526316 0.1        0.05263158 0.1\n",
      " 0.05263158 0.1        0.1        0.1        0.1        0.1\n",
      " 0.05263158 0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.05263158 0.1\n",
      " 0.1        0.05263158 0.1        0.1        0.1        0.1\n",
      " 0.05263158 0.05263158 0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.10526316 0.1        0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.10526316 0.1\n",
      " 0.26315789 0.1        0.1        0.1        0.05263158 0.1\n",
      " 0.1        0.05263158 0.1        0.15789474 0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.05263158 0.1        0.05263158 0.05263158 0.05263158\n",
      " 0.1        0.05263158 0.1        0.1        0.1        0.26315789\n",
      " 0.05263158 0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.1        0.1        0.05263158 0.1\n",
      " 0.1        0.05263158 0.1        0.1        0.05263158 0.1\n",
      " 0.05263158 0.1        0.1        0.1        0.05263158 0.1\n",
      " 0.1        0.05263158 0.1        0.1        0.1        0.15789474\n",
      " 0.05263158 0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.05263158 0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.05263158 0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.05263158 0.1\n",
      " 0.05263158 0.1        0.1        0.1        0.05263158 0.1\n",
      " 0.05263158 0.05263158 0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.1\n",
      " 0.1        0.10526316 0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.05263158 0.1        0.1        0.15789474 0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.10526316 0.05263158 0.1        0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.05263158 0.10526316 0.05263158\n",
      " 0.1        0.05263158 0.10526316 0.05263158 0.05263158 0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.10526316 0.1        0.1        0.1        0.1\n",
      " 0.05263158 0.05263158 0.1        0.1        0.05263158 0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.05263158\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.05263158 0.1\n",
      " 0.05263158 0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.05263158 0.05263158 0.1        0.1        0.10526316\n",
      " 0.1        0.1        0.1        0.1        0.05263158 0.05263158\n",
      " 0.1        0.05263158 0.05263158 0.1        0.1        0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.1        0.15789474\n",
      " 0.1        0.05263158 0.10526316 0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.05263158 0.1        0.21052632]\n"
     ]
    }
   ],
   "source": [
    "print(class1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each test sentence, creating the posterior distribution over the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=np.array(test.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_prob(x,class_no):\n",
    "    prob=1\n",
    "    for i in x:\n",
    "        prob=prob*class_no[i]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predicted=[]\n",
    "confid=[]\n",
    "for x in test.text:\n",
    "    words=preprocess(x)\n",
    "    words_vec=word_to_vocabulary(words)\n",
    "    class1_pred=pr_business*cal_prob(words_vec,class1)\n",
    "    class2_pred=pr_covid*cal_prob(words_vec,class2)\n",
    "    class3_pred=pr_science*cal_prob(words_vec,class3)\n",
    "    class4_pred=pr_sports*cal_prob(words_vec,class4)\n",
    "    class_total=class1_pred+class2_pred+class3_pred+class4_pred\n",
    "    v=np.argmax([class1_pred,class2_pred,class3_pred,class4_pred])\n",
    "    if v==0:\n",
    "        class_predicted.append('business')\n",
    "        confid.append(class1_pred/class_total)\n",
    "    if v==1:\n",
    "        class_predicted.append('covid')\n",
    "        confid.append(class2_pred/class_total)\n",
    "    if v==2:\n",
    "        class_predicted.append('science')\n",
    "        confid.append(class3_pred/class_total)\n",
    "    if v==3:\n",
    "        class_predicted.append('sports')\n",
    "        confid.append(class4_pred/class_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=class_predicted==labels\n",
    "result=list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predicted=result.count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=correct_predicted/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8090614886731392,\n",
       " 0.44096023707440885,\n",
       " 0.9148458446738739,\n",
       " 0.548119086083545,\n",
       " 0.27999999999999997,\n",
       " 0.6535222569296971,\n",
       " 0.31831419128955213,\n",
       " 0.42857142857142855,\n",
       " 0.28965517241379307,\n",
       " 0.4347945518089499,\n",
       " 0.9724543929646967,\n",
       " 0.408371618172537,\n",
       " 0.3169181890389198,\n",
       " 0.293598233995585,\n",
       " 0.9720477892379843,\n",
       " 0.6602839220864972,\n",
       " 0.2914591538212116,\n",
       " 0.3579325698532964,\n",
       " 0.47413704615734104,\n",
       " 0.5906674542232723]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(filtered_sentence)\n",
    "print(len(vectorizer.vocabulary_))\n",
    "vocabulary=vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z(x):\n",
    "    x=x.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    x = [w.translate(table) for w in x]\n",
    "    x = [word.lower() for word in x]\n",
    "    final = [] \n",
    "    for w in x: \n",
    "        if w not in stop_words: \n",
    "            final.append(w)\n",
    "    x=np.array(final)\n",
    "    filtered_sentence_final=[]\n",
    "    for word in x:\n",
    "        if not word.isdigit():\n",
    "            filtered_sentence_final.append(word)\n",
    "    x=np.unique(filtered_sentence_final)\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(x)\n",
    "    return vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(train.category):\n",
    "    x=train.text[i]\n",
    "    words=z(x)\n",
    "    words_vec=word_to_vocabulary(words)\n",
    "    if j==classes[0]:\n",
    "        for x in words_vec:\n",
    "            class1[x]=class1[x]+1\n",
    "    if j==classes[1]:\n",
    "        for x in words_vec:\n",
    "            class2[x]=class2[x]+1\n",
    "    if j==classes[2]:\n",
    "        for x in words_vec:\n",
    "            class3[x]=class3[x]+1\n",
    "    if j==classes[3]:\n",
    "        for x in words_vec:\n",
    "            class4[x]=class4[x]+1\n",
    "class1=class1/business\n",
    "class2=class2/covid\n",
    "class3=class3/science\n",
    "class4=class4/sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1[class1==0] = 0.1\n",
    "class2[class2==0] = 0.1\n",
    "class3[class3==0] = 0.1\n",
    "class4[class4==0] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predicted=[]\n",
    "confid=[]\n",
    "for x in test.text:\n",
    "    words=z(x)\n",
    "    words_vec=word_to_vocabulary(words)\n",
    "    class1_pred=pr_business*cal_prob(words_vec,class1)\n",
    "    class2_pred=pr_covid*cal_prob(words_vec,class2)\n",
    "    class3_pred=pr_science*cal_prob(words_vec,class3)\n",
    "    class4_pred=pr_sports*cal_prob(words_vec,class4)\n",
    "    class_total=class1_pred+class2_pred+class3_pred+class4_pred\n",
    "    v=np.argmax([class1_pred,class2_pred,class3_pred,class4_pred])\n",
    "    #print(class1_pred/class_total,class2_pred/class_total,class3_pred/class_total,class4_pred/class_total)\n",
    "    if v==0:\n",
    "        class_predicted.append('business')\n",
    "        confid.append(class1_pred/class_total)\n",
    "    if v==1:\n",
    "        class_predicted.append('covid')\n",
    "        confid.append(class2_pred/class_total)\n",
    "    if v==2:\n",
    "        class_predicted.append('science')\n",
    "        confid.append(class3_pred/class_total)\n",
    "    if v==3:\n",
    "        class_predicted.append('sports')\n",
    "        confid.append(class4_pred/class_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=class_predicted==labels\n",
    "result=list(result)\n",
    "correct_predicted=result.count(True)\n",
    "accuracy=correct_predicted/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999996754159474,\n",
       " 0.7568836392641031,\n",
       " 0.997989357786861,\n",
       " 0.9999999931743092,\n",
       " 0.9963204885270126,\n",
       " 0.9999999999996689,\n",
       " 1.0,\n",
       " 0.9999999999227952,\n",
       " 0.9989587833707586,\n",
       " 0.9999968017304589,\n",
       " 1.0,\n",
       " 0.9999615863351414,\n",
       " 0.9928664213119752,\n",
       " 0.9995695174442786,\n",
       " 1.0,\n",
       " 0.9100388226300743,\n",
       " 0.9999887100889951,\n",
       " 0.9999934871310998,\n",
       " 0.9999967239610472,\n",
       " 0.9980410740344783]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"40.csv\")\n",
    "test= pd.read_csv(\"10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the midst of the COVID-19 pandemic, eating ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>While there are no specific foods that can hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You may not be able to share meals with friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eating a healthy diet is not about strict limi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healthy eating doesn’t have to be overly compl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  In the midst of the COVID-19 pandemic, eating ...\n",
       "1  While there are no specific foods that can hel...\n",
       "2  You may not be able to share meals with friend...\n",
       "3  Eating a healthy diet is not about strict limi...\n",
       "4  Healthy eating doesn’t have to be overly compl..."
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eating healthy food is important for maintainn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Following a healthy diet will boost your _____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avoid eating chemical additives, added sugars ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Your diet should be rich of vitamins D,K, calc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The healthier the food you eat, the better you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Eating healthy food is important for maintainn...\n",
       "1     Following a healthy diet will boost your _____\n",
       "2  Avoid eating chemical additives, added sugars ...\n",
       "3  Your diet should be rich of vitamins D,K, calc...\n",
       "4  The healthier the food you eat, the better you..."
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  0           In\n",
      "   1          the\n",
      "   2        midst\n",
      "   3           of\n",
      "   4          the\n",
      "   5     COVID-19\n",
      "   6    pandemic,\n",
      "   7       eating\n",
      "   8      healthy\n",
      "   9         food\n",
      "dtype: object\n",
      "['In' 'the' 'midst' 'of' 'the' 'COVID-19' 'pandemic,' 'eating' 'healthy'\n",
      " 'food']\n"
     ]
    }
   ],
   "source": [
    "u = train.text.str.split(expand=True).stack()\n",
    "\n",
    "print(u[:10])\n",
    "\n",
    "words=u.values\n",
    "\n",
    "print(words[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'midst', 'of', 'the', 'COVID19', 'pandemic', 'eating', 'healthy', 'food', 'remains', 'an', 'important', 'part', 'of', 'maintaining', 'your', 'health', 'While', 'there', 'are', 'no', 'specific', 'foods', 'that', 'can', 'help', 'protect', 'you', 'from', 'the', 'virus', 'a', 'nutritious', 'diet', 'can', 'boost', 'your', 'immune', 'system', 'or', 'help', 'you', 'fight', 'off', 'symptoms', 'You', 'may', 'not', 'be', 'able', 'to', 'share', 'meals', 'with', 'friends', 'and', 'loved', 'ones', 'but', 'there', 'are', 'lots', 'of', 'other', 'ways', 'to', 'eat', 'well', 'and', 'support', 'your', 'health', 'at', 'this', 'difficult', 'time', 'Eating', 'a', 'healthy', 'diet', 'is', 'not', 'about', 'strict', 'limitations', 'staying', 'unrealistically', 'thin', 'or', 'depriving', 'yourself', 'of', 'the', 'foods', 'you', 'love', 'Rather', 'it’s', 'about']\n"
     ]
    }
   ],
   "source": [
    "words=words.tolist()\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "final = [w.translate(table) for w in words]\n",
    "\n",
    "print(final[:100])\n",
    "\n",
    "words = [word.lower() for word in final]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577\n",
      "377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in words: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(len(filtered_sentence))\n",
    "\n",
    "filtered_sentence2=np.unique(np.array(filtered_sentence))\n",
    "\n",
    "print(len(filtered_sentence2))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able' 'add' 'added' 'adding' 'additives' 'advice' 'age' 'alone' 'also'\n",
      " 'alternatives' 'always' 'amount' 'amounts' 'antioxidants' 'anxiety'\n",
      " 'around' 'asparagus' 'avoid' 'aware' 'back' 'bacon' 'bad' 'balance'\n",
      " 'beans' 'become' 'beets' 'bell' 'better' 'bland' 'bloated' 'blood' 'body'\n",
      " 'body’s' 'boiling' 'boost' 'boosting' 'boredom' 'box' 'brain' 'bread'\n",
      " 'breakfast' 'broccoli' 'brussels' 'buildup' 'calcium' 'calciumrich'\n",
      " 'calories' 'carbohydrates' 'carbs' 'carrots' 'certain' 'change' 'changes'\n",
      " 'charge' 'cheating' 'chemical' 'chicken' 'chili' 'choices' 'claiming'\n",
      " 'cognitive' 'come' 'common' 'completely' 'complex' 'complicated'\n",
      " 'computer' 'conflicting' 'considered' 'continue' 'contribute' 'control'\n",
      " 'cook' 'cooking' 'cope' 'could' 'covid19' 'cravings' 'cut' 'cutting'\n",
      " 'daily' 'dangerous' 'day' 'dehydrated—causing' 'dense' 'deplete'\n",
      " 'depression' 'depriving' 'diabetes' 'diet' 'dietary' 'diets' 'difference'\n",
      " 'difficult' 'difficulties' 'dinner—but' 'disease' 'diseases' 'dishes'\n",
      " 'doesn’t' 'donuts' 'don’t' 'drained' 'drink' 'eat' 'eating' 'eliminate'\n",
      " 'eliminating' 'emotional' 'emotions' 'end' 'energy' 'enjoy' 'enough'\n",
      " 'especially' 'essence' 'even' 'everything' 'exacerbate' 'exactly'\n",
      " 'example' 'fact' 'fat' 'fats' 'fats—such' 'feel' 'feeling' 'fewer'\n",
      " 'fiber' 'fight' 'fill' 'five' 'flakes' 'fluctuations' 'flush' 'focus'\n",
      " 'follow' 'food' 'foods' 'fried' 'friends' 'front' 'fruit' 'frying'\n",
      " 'function' 'garlic' 'gender' 'get' 'getting' 'gives' 'giving' 'go' 'goes'\n",
      " 'going—while' 'good' 'go—and' 'grains' 'great' 'green' 'grilled'\n",
      " 'grilling' 'habit' 'harmful' 'headaches' 'health' 'healthier' 'healthy'\n",
      " 'heart' 'help' 'helps' 'hide' 'high' 'highquality' 'home' 'hunger'\n",
      " 'hydrated' 'immune' 'important' 'improve' 'improving' 'include'\n",
      " 'including' 'increase' 'instead' 'irritable' 'it’s' 'job' 'junk' 'k'\n",
      " 'keep' 'kidney' 'large' 'latest' 'leading' 'leads' 'least' 'leave' 'less'\n",
      " 'life' 'likely']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_sentence2[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=0\n",
    "vocabulary={}\n",
    "for word in filtered_sentence2:\n",
    "    vocabulary[word]=z\n",
    "    z=z+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prior probabilities of all \"labels\", i.e. words in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior=[0]*len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in filtered_sentence:\n",
    "    prior[vocabulary[word]] =prior[vocabulary[word]]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior=np.array(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts_in_class=prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 1, 1, 1, 2, 2, 5, 1])"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_counts_in_class[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior=prior/577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class-conditional probabilities of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_counts_in_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts_in_class=[total_counts_in_class]*len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts_in_class=np.array(total_counts_in_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377, 377)"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_counts_in_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 2, ..., 1, 2, 2],\n",
       "       [1, 2, 2, ..., 1, 2, 2],\n",
       "       [1, 2, 2, ..., 1, 2, 2],\n",
       "       ...,\n",
       "       [1, 2, 2, ..., 1, 2, 2],\n",
       "       [1, 2, 2, ..., 1, 2, 2],\n",
       "       [1, 2, 2, ..., 1, 2, 2]])"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_counts_in_class[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prob=np.zeros((len(vocabulary),len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x=x.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    x = [w.translate(table) for w in x]\n",
    "    x = [word.lower() for word in x]\n",
    "    final = [] \n",
    "    for w in x: \n",
    "        if w not in stop_words: \n",
    "            final.append(w)\n",
    "    x=np.array(final)\n",
    "    x=np.unique(x)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vocabulary(x):\n",
    "    final=[]\n",
    "    for word in x:\n",
    "        if word in vocabulary:\n",
    "            final.append(vocabulary[word])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in train.text:\n",
    "    x=sent\n",
    "    words=preprocess(x)\n",
    "    words_vec=word_to_vocabulary(words)\n",
    "    for i in words_vec:\n",
    "        class_prob[i][words_vec]=class_prob[i][words_vec]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prob=class_prob/total_counts_in_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 1. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 1. , ..., 0. , 0.5, 0. ],\n",
       "       ...,\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0.5],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ]])"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_prob[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replacing zero probability to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prob[class_prob==0]=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 1. , 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 1. , ..., 0.1, 0.5, 0.1],\n",
       "       ...,\n",
       "       [0.1, 0.1, 0.1, ..., 1. , 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.5, ..., 0.1, 1. , 0.1],\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 1. ]])"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculating the most likely word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_to_word=dict([(y,x) for x,y in vocabulary.items()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cut'"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_to_word[78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob=[]\n",
    "y_pred=[]\n",
    "for sent in test.text:\n",
    "    x=preprocess(sent)\n",
    "    words=word_to_vocabulary(x)\n",
    "    pred=1\n",
    "    for i in words:\n",
    "        pred=pred*class_prob[:,i]\n",
    "    pred=prior*pred\n",
    "    pred2=np.argmax(pred)\n",
    "    prob.append(pred[pred2])\n",
    "    y_pred.append(vector_to_word[pred2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['healthy', 'diet', 'foods', 'diet', 'food', 'energy', 'foods', 'foods', 'eating', 'food']\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00016175621028307339, 0.007427581084426837, 1.6880866100970088e-05, 0.0038513383400731746, 0.0020797227036395147, 0.0025996533795493936, 3.2495667244367427e-06, 1.719347473246953e-05, 0.0010398613518197576, 0.0011554015020219526]\n"
     ]
    }
   ],
   "source": [
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eating healthy food is important for maintainng good _____'"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Eating healthy food is important for maintainn...\n",
       "1       Following a healthy diet will boost your _____\n",
       "2    Avoid eating chemical additives, added sugars ...\n",
       "3    Your diet should be rich of vitamins D,K, calc...\n",
       "4    The healthier the food you eat, the better you...\n",
       "5    Dehydration causes tiredness, low energy and h...\n",
       "6    People with kidney disease should avoid eating...\n",
       "7    We should avoid eating transfats. Eating healt...\n",
       "8    Mindless eating is often caused by eating alon...\n",
       "9    Eating more junk food will make you feel uncom...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
